{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tarar\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\tarar\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\tarar\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data related libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Parameters handling libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = \"tara\"\n",
    "ACTIVITIES = [\"relaxing\", \"eating\", \"walking\", \"studying\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(dataset: pd.DataFrame, dataset_identifier: str):\n",
    "    # Check if there are any remaining missing values\n",
    "    missing_values = dataset.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"There are still {missing_values} missing values in the [{dataset_identifier}]\")\n",
    "    else:\n",
    "        print(f\"All missing values have been filled in the [{dataset_identifier}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_data(dataset: pd.DataFrame):\n",
    "    # Forward fill\n",
    "    dataset = dataset.fillna(method='ffill')  \n",
    "    # backward fill missing values\n",
    "    dataset = dataset.fillna(method='bfill')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are still 1260 missing values in the [train_dataset]\n",
      "All missing values have been filled in the [train_dataset]\n",
      "train_y (21596, 4)\n",
      "train_x (21596, 462)\n",
      "There are still 1260 missing values in the [test_dataset]\n",
      "All missing values have been filled in the [test_dataset]\n",
      "test_y (3545, 4)\n",
      "test_x (3545, 462)\n"
     ]
    }
   ],
   "source": [
    "source_directory = \"final_datasets\"\n",
    "\n",
    "# Loading `train` data\n",
    "train_dataset = pd.read_csv(f\"{source_directory}/{EXPERIMENT}_train_features.csv\")\n",
    "train_dataset = train_dataset.drop(['time'], axis=1)\n",
    "check_missing_values(train_dataset, \"train_dataset\")\n",
    "train_dataset = fill_missing_data(train_dataset)\n",
    "check_missing_values(train_dataset, \"train_dataset\")\n",
    "train_y = train_dataset[ACTIVITIES]\n",
    "print(\"train_y\", train_y.shape)\n",
    "train_x = train_dataset.drop(ACTIVITIES, axis=1)\n",
    "print(\"train_x\", train_x.shape)\n",
    "\n",
    "# Loading `test` data\n",
    "test_dataset = pd.read_csv(f\"{source_directory}/{EXPERIMENT}_test_features.csv\")\n",
    "test_dataset = test_dataset.drop(['time'], axis=1)\n",
    "check_missing_values(test_dataset, \"test_dataset\")\n",
    "test_dataset = fill_missing_data(test_dataset)\n",
    "check_missing_values(test_dataset, \"test_dataset\")\n",
    "test_y = test_dataset[ACTIVITIES]\n",
    "print(\"test_y\", test_y.shape)\n",
    "test_x = test_dataset.drop(ACTIVITIES, axis=1)\n",
    "print(\"test_x\", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grid_search_result(model_instance, parameters, X, y):\n",
    "    grid_search_instance = GridSearchCV(model_instance, parameters)\n",
    "    return grid_search_instance.fit(X, y)\n",
    "\n",
    "def compute_metrics(test_y: np.ndarray, predicted_y: np.ndarray):\n",
    "    accuracy_score = metrics.accuracy_score(test_y, predicted_y)\n",
    "    precision_score = metrics.precision_score(test_y, predicted_y, average=None)\n",
    "    recall_score = metrics.recall_score(test_y, predicted_y, average=None)\n",
    "    f1_score = metrics.f1_score(test_y, predicted_y, average=None)\n",
    "    roc_auc_score = metrics.roc_auc_score(test_y, predicted_y, average=None)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy_score\": accuracy_score,\n",
    "        \"precision_score\": precision_score,\n",
    "        \"recall_score\": recall_score,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"roc_auc_score\": roc_auc_score,\n",
    "    }\n",
    "\n",
    "def add_new_key_value_to_dict(dict, key, value):\n",
    "    dict[key] = value\n",
    "    return dict\n",
    "\n",
    "def fit_predict_and_compute_metrics(\n",
    "        model_definition: type,\n",
    "        parameters: dict,\n",
    "        train_x: pd.DataFrame,\n",
    "        train_y: pd.DataFrame,\n",
    "        test_x: pd.DataFrame,\n",
    "        test_y: pd.DataFrame\n",
    "    ):\n",
    "\n",
    "    model_instance: object = model_definition(**parameters)\n",
    "    model_instance.fit(train_x, train_y)\n",
    "    predicted_y: np.ndarray = model_instance.predict(test_x)\n",
    "    metrics_result: dict = compute_metrics(test_y.values, predicted_y)\n",
    "    return add_new_key_value_to_dict(metrics_result, \"parameters\", parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results: dict, model_name: str):\n",
    "    scores = {\n",
    "        'Precision': results['precision_score'],\n",
    "        'Recall': results['recall_score'],\n",
    "        'F1-score': results['f1_score'],\n",
    "        'Roc_auc_score': results['roc_auc_score']\n",
    "    }\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "    bar_width = 0.2\n",
    "    opacity = 0.8\n",
    "\n",
    "    for i, (score_name, score_values) in enumerate(scores.items()):\n",
    "        x = np.arange(len(score_values))\n",
    "        bar_offset = i * bar_width\n",
    "        ax.bar(x + bar_offset, score_values, bar_width, alpha=opacity, label=score_name)\n",
    "\n",
    "    ax.set_xlabel('Activity')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(f'{model_name} Performance Metrics')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(ACTIVITIES)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    for activity, score_values in zip(ACTIVITIES, zip(*scores.values())):\n",
    "        print(f'{activity}: {dict(zip(scores.keys(), score_values))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_parameters = {\n",
    "    \"max_depth\": [15],\n",
    "    \"n_estimators\" : [100],\n",
    "    \"min_samples_split\" : [7]\n",
    "}\n",
    "\n",
    "random_forest_grid_search_result = compute_grid_search_result(\n",
    "    RandomForestClassifier(),\n",
    "    random_forest_parameters,\n",
    "    train_x,\n",
    "    train_y\n",
    ")\n",
    "\n",
    "print(f\"Best parameters = {random_forest_grid_search_result.best_params_}\\n\\n\")\n",
    "\n",
    "random_forest_result: dict = fit_predict_and_compute_metrics(\n",
    "    RandomForestClassifier,\n",
    "    random_forest_grid_search_result.best_params_,\n",
    "    train_x,\n",
    "    train_y,\n",
    "    test_x,\n",
    "    test_y\n",
    ")\n",
    "    \n",
    "        \n",
    "plot_results(random_forest_result, \"RandomForestClassifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_parameters = {\n",
    "    'max_depth': [3],\n",
    "    'n_estimators': [25],\n",
    "    'learning_rate': [0.2],\n",
    "}\n",
    "\n",
    "xgb_grid_search_result = compute_grid_search_result(\n",
    "    XGBClassifier(),\n",
    "    svm_parameters,\n",
    "    train_x,\n",
    "    train_y\n",
    ")\n",
    "print(f\"Best parameters = {xgb_grid_search_result.best_params_}\\n\\n\")\n",
    "\n",
    "xgb_result: dict = fit_predict_and_compute_metrics(\n",
    "    XGBClassifier,\n",
    "    xgb_grid_search_result.best_params_,\n",
    "    train_x,\n",
    "    train_y,\n",
    "    test_x,\n",
    "    test_y\n",
    ")\n",
    "plot_results(xgb_result, \"XGBClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
